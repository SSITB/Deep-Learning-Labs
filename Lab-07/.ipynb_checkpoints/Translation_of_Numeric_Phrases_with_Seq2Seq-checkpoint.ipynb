{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Translation of Numeric Phrases with Seq2Seq\n",
    "\n",
    "In the following we will try to build a **translation model from french phrases describing numbers** to the corresponding **numeric representation** (base 10).\n",
    "\n",
    "This is a toy machine translation task with a **restricted vocabulary** and a **single valid translation for each source phrase** which makes it more tractable to train on a laptop computer and easier to evaluate. Despite those limitations we expect that this task will highlight interesting properties of Seq2Seq models including:\n",
    "\n",
    "- the ability to **deal with different length** of the source and target sequences,\n",
    "- handling token with a **meaning that changes depending on the context** (e.g \"quatre\" vs \"quatre vingts\" in \"quatre cents\"),\n",
    "- basic counting and \"reasoning\" capabilities of LSTM and GRU models.\n",
    "\n",
    "The parallel text data is generated from a \"ground-truth\" Python function named `to_french_phrase` that captures common rules. Hyphenation was intentionally omitted to make the phrases more ambiguous and therefore make the translation problem slightly harder to solve (and also because Olivier had no particular interest hyphenation in properly implementing rules :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    21 vingt et un\n",
      "    80 quatre vingts\n",
      "    81 quatre vingt un\n",
      "   300 trois cents\n",
      "   213 deux cent treize\n",
      "  1100 mille cent\n",
      "  1201 mille deux cent un\n",
      "301000 trois cent un mille\n",
      " 80080 quatre vingt mille quatre vingts\n"
     ]
    }
   ],
   "source": [
    "from french_numbers import to_french_phrase\n",
    "\n",
    "for x in [21, 80, 81, 300, 213, 1100, 1201, 301000, 80080]:\n",
    "    print(str(x).rjust(6), to_french_phrase(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Generating a Training Set\n",
    "\n",
    "The following will **generate phrases 20000 example phrases for numbers between 1 and 1,000,000** (excluded). We chose to over-represent small numbers by generating all the possible short sequences between `1` and `exhaustive=5000`.\n",
    "\n",
    "We then split the generated set into non-overlapping train, validation and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from french_numbers import generate_translations\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "numbers, french_numbers = generate_translations(\n",
    "    low=1, high=int(1e6) - 1, exhaustive=5000, random_seed=0)\n",
    "num_train, num_dev, fr_train, fr_dev = train_test_split(\n",
    "    numbers, french_numbers, test_size=0.5, random_state=0)\n",
    "\n",
    "num_val, num_test, fr_val, fr_test = train_test_split(\n",
    "    num_dev, fr_dev, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 5000, 5000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fr_train), len(fr_val), len(fr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2882 deux mille huit cent quatre vingt deux\n",
      "372200 trois cent soixante douze mille deux cents\n",
      "  2193 deux mille cent quatre vingt treize\n",
      "996418 neuf cent quatre vingt seize mille quatre cent dix huit\n",
      "  9172 neuf mille cent soixante douze\n"
     ]
    }
   ],
   "source": [
    "for i, fr_phrase, num_phrase in zip(range(5), fr_train, num_train):\n",
    "    print(num_phrase.rjust(6), fr_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2804 deux mille huit cent quatre\n",
      "  3898 trois mille huit cent quatre vingt dix huit\n",
      " 82996 quatre vingt deux mille neuf cent quatre vingt seize\n",
      "366346 trois cent soixante six mille trois cent quarante six\n",
      " 56006 cinquante six mille six\n"
     ]
    }
   ],
   "source": [
    "for i, fr_phrase, num_phrase in zip(range(5), fr_val, num_val):\n",
    "    print(num_phrase.rjust(6), fr_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vocabularies\n",
    "\n",
    "Build the vocabularies from the training set only to get a chance to have some out-of-vocabulary words in the validation and test sets.\n",
    "\n",
    "First we need to introduce specific symbols that will be used to:\n",
    "- pad sequences\n",
    "- mark the beginning of translation\n",
    "- mark the end of translation\n",
    "- be used as a placehold for out-of-vocabulary symbols (not seen in the training set).\n",
    "\n",
    "Here we use the same convention as the [tensorflow seq2seq tutorial](https://www.tensorflow.org/tutorials/seq2seq):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "PAD, GO, EOS, UNK = START_VOCAB = ['_PAD', '_GO', '_EOS', '_UNK']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To build the vocabulary we need to tokenize the sequences of symbols. For the digital number representation we use character level tokenization while whitespace-based word level tokenization will do for the French phrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sentence, word_level=True):\n",
    "    if word_level:\n",
    "        return sentence.split()\n",
    "    else:\n",
    "        return [sentence[i:i + 1] for i in range(len(sentence))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3', '4']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('1234', word_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mille', 'deux', 'cent', 'trente', 'quatre']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('mille deux cent trente quatre', word_level=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's now use this tokenization strategy to assign a unique integer token id to each possible token string found the traing set in each language ('French' and 'numeric'): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(tokenized_sequences):\n",
    "    rev_vocabulary = START_VOCAB[:]\n",
    "    unique_tokens = set()\n",
    "    for tokens in tokenized_sequences:\n",
    "        unique_tokens.update(tokens)\n",
    "    rev_vocabulary += sorted(unique_tokens)\n",
    "    vocabulary = {}\n",
    "    for i, token in enumerate(rev_vocabulary):\n",
    "        vocabulary[token] = i\n",
    "    return vocabulary, rev_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tokenized_fr_train = [tokenize(s, word_level=True) for s in fr_train]\n",
    "tokenized_num_train = [tokenize(s, word_level=False) for s in num_train]\n",
    "\n",
    "fr_vocab, rev_fr_vocab = build_vocabulary(tokenized_fr_train)\n",
    "num_vocab, rev_num_vocab = build_vocabulary(tokenized_num_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The two languages do not have the same vocabulary sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_EOS': 2,\n",
       " '_GO': 1,\n",
       " '_PAD': 0,\n",
       " '_UNK': 3,\n",
       " 'cent': 4,\n",
       " 'cents': 5,\n",
       " 'cinq': 6,\n",
       " 'cinquante': 7,\n",
       " 'deux': 8,\n",
       " 'dix': 9,\n",
       " 'douze': 10,\n",
       " 'et': 11,\n",
       " 'huit': 12,\n",
       " 'mille': 13,\n",
       " 'neuf': 14,\n",
       " 'onze': 15,\n",
       " 'quarante': 16,\n",
       " 'quatorze': 17,\n",
       " 'quatre': 18,\n",
       " 'quinze': 19,\n",
       " 'seize': 20,\n",
       " 'sept': 21,\n",
       " 'six': 22,\n",
       " 'soixante': 23,\n",
       " 'treize': 24,\n",
       " 'trente': 25,\n",
       " 'trois': 26,\n",
       " 'un': 27,\n",
       " 'vingt': 28,\n",
       " 'vingts': 29}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fr_vocab)\n",
    "fr_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 4,\n",
       " '1': 5,\n",
       " '2': 6,\n",
       " '3': 7,\n",
       " '4': 8,\n",
       " '5': 9,\n",
       " '6': 10,\n",
       " '7': 11,\n",
       " '8': 12,\n",
       " '9': 13,\n",
       " '_EOS': 2,\n",
       " '_GO': 1,\n",
       " '_PAD': 0,\n",
       " '_UNK': 3}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(num_vocab)\n",
    "num_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      _EOS 2\n",
      "       _GO 1\n",
      "      _PAD 0\n",
      "      _UNK 3\n",
      "      cent 4\n",
      "     cents 5\n",
      "      cinq 6\n",
      " cinquante 7\n",
      "      deux 8\n",
      "       dix 9\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "for k, v in sorted(fr_vocab.items())[:10]:\n",
    "    print(k.rjust(10), v)\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0 4\n",
      "         1 5\n",
      "         2 6\n",
      "         3 7\n",
      "         4 8\n",
      "         5 9\n",
      "         6 10\n",
      "         7 11\n",
      "         8 12\n",
      "         9 13\n",
      "      _EOS 2\n",
      "       _GO 1\n",
      "      _PAD 0\n",
      "      _UNK 3\n"
     ]
    }
   ],
   "source": [
    "for k, v in sorted(num_vocab.items()):\n",
    "    print(k.rjust(10), v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We also built the reverse mappings from token ids to token string representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_PAD', '_GO', '_EOS', '_UNK', 'cent', 'cents', 'cinq', 'cinquante', 'deux', 'dix', 'douze', 'et', 'huit', 'mille', 'neuf', 'onze', 'quarante', 'quatorze', 'quatre', 'quinze', 'seize', 'sept', 'six', 'soixante', 'treize', 'trente', 'trois', 'un', 'vingt', 'vingts']\n"
     ]
    }
   ],
   "source": [
    "print(rev_fr_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_PAD', '_GO', '_EOS', '_UNK', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "print(rev_num_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Seq2Seq with a single GRU architecture\n",
    "\n",
    "<img src=\"https://github.com/imadelh/Deep-Learning-Labs/raw/master/Lab-07/images/basic_seq2seq.png\" width=\"80%\" />\n",
    "\n",
    "From: [Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. \"Sequence to sequence learning with neural networks.\" NIPS 2014](https://arxiv.org/abs/1409.3215)\n",
    "\n",
    "\n",
    "\n",
    "For a given source sequence - target sequence pair, we will:\n",
    "- tokenize the source and target sequences;\n",
    "- reverse the order of the source sequence;\n",
    "- build the input sequence by concatenating the reversed source sequence and the target sequence in original order using the `_GO` token as a delimiter, \n",
    "- build the output sequence by appending the `_EOS` token to the source sequence.\n",
    "\n",
    "\n",
    "Let's do this as a function using the original string representations for the tokens so as to make it easier to debug:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "- Write a function that turn a pair of tokenized (source, target) sequences into a pair of (input, output) sequences as described above.\n",
    "- The function should have a `reverse_source=True` as an option.\n",
    "\n",
    "Notes: \n",
    "- The function should output two sequences of string tokens: one to be fed as the input and the other as expected output for the seq2seq network.\n",
    "- Do not pad the sequences: we will handle the padding later.\n",
    "- Don't forget to insert the `_GO` and `_EOS` special symbols at the right locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_input_output(source_tokens, target_tokens, reverse_source=True):\n",
    "    input_tokens = []\n",
    "    output_tokens = []\n",
    "    if reverse_source:\n",
    "        source_tokens =list(reversed(source_tokens))\n",
    "    for i in source_tokens:\n",
    "        input_tokens.append(i)\n",
    "    input_tokens.append('_GO')\n",
    "    for j in target_tokens:\n",
    "        input_tokens.append(j)\n",
    "        output_tokens.append(j)\n",
    "    output_tokens.append('_EOS')\n",
    "    return input_tokens, output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_tokens, output_tokens = make_input_output(\n",
    "    ['cent', 'vingt', 'et', 'un'],\n",
    "    ['1', '2', '1'],\n",
    ")\n",
    "# Expected outputs:\n",
    "# ['un', 'et', 'vingt', 'cent', '_GO', '1', '2', '1']\n",
    "# ['1', '2', '1', '_EOS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['un', 'et', 'vingt', 'cent', '_GO', '1', '2', '1']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '1', '_EOS']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Vectorization of the parallel corpus\n",
    "\n",
    "Let's apply the previous transformation to each pair of (source, target) sequene and use a shared vocabulary to store the results in numpy arrays of integer token ids, with padding on the left so that all input / output sequences have the same length: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_tokenized_sequences = tokenized_fr_train + tokenized_num_train\n",
    "shared_vocab, rev_shared_vocab = build_vocabulary(all_tokenized_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(s) for s in tokenized_fr_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(s) for s in tokenized_num_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "max_length = 20  # found by introspection of our training set\n",
    "\n",
    "def vectorize_corpus(source_sequences, target_sequences, shared_vocab,\n",
    "                     word_level_source=True, word_level_target=True,\n",
    "                     max_length=max_length):\n",
    "    assert len(source_sequences) == len(target_sequences)\n",
    "    n_sequences = len(source_sequences)\n",
    "    source_ids = np.empty(shape=(n_sequences, max_length), dtype=np.int32)\n",
    "    source_ids.fill(shared_vocab[PAD])\n",
    "    target_ids = np.empty(shape=(n_sequences, max_length), dtype=np.int32)\n",
    "    target_ids.fill(shared_vocab[PAD])\n",
    "    numbered_pairs = zip(range(n_sequences), source_sequences, target_sequences)\n",
    "    for i, source_seq, target_seq in numbered_pairs:\n",
    "        source_tokens = tokenize(source_seq, word_level=word_level_source)\n",
    "        target_tokens = tokenize(target_seq, word_level=word_level_target)\n",
    "        \n",
    "        in_tokens, out_tokens = make_input_output(source_tokens, target_tokens)\n",
    "        \n",
    "        in_token_ids = [shared_vocab.get(t, UNK) for t in in_tokens]\n",
    "        source_ids[i, -len(in_token_ids):] = in_token_ids\n",
    "    \n",
    "        out_token_ids = [shared_vocab.get(t, UNK) for t in out_tokens]\n",
    "        target_ids[i, -len(out_token_ids):] = out_token_ids\n",
    "    return source_ids, target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = vectorize_corpus(fr_train, num_train, shared_vocab,\n",
    "                                    word_level_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 20)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 20)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deux mille huit cent quatre vingt deux'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2882'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0, 18, 38, 28, 14, 22, 23, 18,  1,  6,\n",
       "       12, 12,  6], dtype=int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  6, 12,\n",
       "       12,  6,  2], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This looks good. In particular we can note:\n",
    "\n",
    "- the PAD=0 symbol at the beginning of the two sequences,\n",
    "- the input sequence has the GO=1 symbol to separate the source from the target,\n",
    "- the output sequence is a shifted version of the target and ends with EOS=2.\n",
    "\n",
    "Let's vectorize the validation and test set to be able to evaluate our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_val, Y_val = vectorize_corpus(fr_val, num_val, shared_vocab,\n",
    "                                word_level_target=False)\n",
    "X_test, Y_test = vectorize_corpus(fr_test, num_test, shared_vocab,\n",
    "                                  word_level_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 20), (5000, 20))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 20), (5000, 20))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### A simple homogeneous Seq2Seq architecture\n",
    "\n",
    "To keep the architecture simple we will use the **same RNN model and weights for both the encoder part** (before the `_GO` token) **and the decoder part** (after the `_GO` token).\n",
    "\n",
    "We may GRU recurrent cell instead of LSTM because it is slightly faster to compute and should give comparable results.\n",
    "\n",
    "**Exercise:**\n",
    "- Build a Seq2Seq model:\n",
    "  - Start with an Embedding layer;\n",
    "  - Add a single GRU layer: the GRU layer should yield a sequence of output vectors, one at each timestep;\n",
    "  - Add a Dense layer to adapt the ouput dimension of the GRU layer to the dimension of the output vocabulary;\n",
    "  - Don't forget to insert some Dropout layer(s), especially after the Embedding layer.\n",
    "\n",
    "Note:\n",
    "- The output dimension of the Embedding layer should be smaller than usual be cause we have small vocabulary size;\n",
    "- The dimension of the GRU should be larger to give the Seq2Seq model enough \"working memory\" to memorize the full input sequence before decoding it;\n",
    "- Your model should output a shape `[batch, sequence_length, vocab_size]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(shared_vocab)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dropout, GRU, Dense\n",
    "\n",
    "from keras.layers import Input, Flatten\n",
    "\n",
    "EMBEDDING_DIM = 10\n",
    "units = 100\n",
    "\n",
    "vocab_size = len(shared_vocab)\n",
    "\n",
    "simple_seq2seq = Sequential()\n",
    "\n",
    "#simple_seq2seq.add(Input(shape=(max_length,)))\n",
    "simple_seq2seq.add(Embedding(vocab_size, EMBEDDING_DIM,\n",
    "                            input_length=max_length,\n",
    "                            trainable=True))\n",
    "simple_seq2seq.add(Dropout(0.2))\n",
    "\n",
    "simple_seq2seq.add(GRU(units, return_sequences = True))\n",
    "\n",
    "#simple_seq2seq.add(Flatten())\n",
    "\n",
    "simple_seq2seq.add(Dense(vocab_size,activation = \"softmax\"))\n",
    "\n",
    "\n",
    "# TODO\n",
    "\n",
    "# Here we use the sparse_categorical_crossentropy loss to be able to pass\n",
    "# integer-coded output for the token ids without having to convert to one-hot\n",
    "# codes\n",
    "simple_seq2seq.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 20, 10)            400       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20, 10)            0         \n",
      "_________________________________________________________________\n",
      "gru_14 (GRU)                 (None, 20, 100)           33300     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 20, 40)            4040      \n",
      "=================================================================\n",
      "Total params: 37,740\n",
      "Trainable params: 37,740\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "simple_seq2seq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dropout, GRU, Dense\n",
    "\n",
    "vocab_size = len(shared_vocab)\n",
    "simple_seq2seq = Sequential()\n",
    "simple_seq2seq.add(Embedding(vocab_size, 32, input_length=max_length))\n",
    "simple_seq2seq.add(Dropout(0.2)) ## We will see that on the training set, the prediction is going to be harder but better on testing set..\n",
    "simple_seq2seq.add(GRU(256, return_sequences=True))\n",
    "#simple_seq2seq.add(GRU(256, return_sequences=True))\n",
    "simple_seq2seq.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "# Here we use the sparse_categorical_crossentropy loss to be able to pass\n",
    "# integer-coded output for the token ids without having to convert to one-hot\n",
    "# codes\n",
    "simple_seq2seq.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "\n",
    "- What is the expected shape of the output of the model when fed with input of length 20 tokens? What is the meaning of the values in the output of the model?\n",
    "- What is the shape of the output of each layer in the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20, 40)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_seq2seq.predict(X_train[0:1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 20, 32)            1280      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 20, 32)            0         \n",
      "_________________________________________________________________\n",
      "gru_15 (GRU)                 (None, 20, 256)           221952    \n",
      "_________________________________________________________________\n",
      "gru_16 (GRU)                 (None, 20, 256)           393984    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 20, 40)            10280     \n",
      "=================================================================\n",
      "Total params: 627,496\n",
      "Trainable params: 627,496\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "simple_seq2seq.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's register a callback mechanism to automatically snapshot the best model by measure the performance of the model on the validation set at the end of each epoch during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "best_model_fname = \"simple_seq2seq_checkpoint.h5\"\n",
    "best_model_cb = ModelCheckpoint(best_model_fname, monitor='val_loss',\n",
    "                                save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We need to use np.expand_dims trick on Y: this is required by Keras because of we use a sparse (integer-based) representation for the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/15\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55932, saving model to simple_seq2seq_checkpoint.h5\n",
      " - 20s - loss: 0.7300 - val_loss: 0.5593\n",
      "Epoch 2/15\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55932 to 0.48297, saving model to simple_seq2seq_checkpoint.h5\n",
      " - 18s - loss: 0.5273 - val_loss: 0.4830\n",
      "Epoch 3/15\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.48297 to 0.42117, saving model to simple_seq2seq_checkpoint.h5\n",
      " - 18s - loss: 0.4727 - val_loss: 0.4212\n",
      "Epoch 4/15\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.42117 to 0.32959, saving model to simple_seq2seq_checkpoint.h5\n",
      " - 19s - loss: 0.3857 - val_loss: 0.3296\n",
      "Epoch 5/15\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.32959 to 0.27021, saving model to simple_seq2seq_checkpoint.h5\n",
      " - 19s - loss: 0.3218 - val_loss: 0.2702\n",
      "Epoch 6/15\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.27021 to 0.24711, saving model to simple_seq2seq_checkpoint.h5\n",
      " - 18s - loss: 0.2574 - val_loss: 0.2471\n",
      "Epoch 7/15\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.24711 to 0.17577, saving model to simple_seq2seq_checkpoint.h5\n",
      " - 18s - loss: 0.2045 - val_loss: 0.1758\n",
      "Epoch 8/15\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.17577 to 0.12603, saving model to simple_seq2seq_checkpoint.h5\n",
      " - 18s - loss: 0.1709 - val_loss: 0.1260\n",
      "Epoch 9/15\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.12603 to 0.09037, saving model to simple_seq2seq_checkpoint.h5\n",
      " - 18s - loss: 0.1198 - val_loss: 0.0904\n",
      "Epoch 10/15\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.09037 to 0.07017, saving model to simple_seq2seq_checkpoint.h5\n",
      " - 18s - loss: 0.0911 - val_loss: 0.0702\n",
      "Epoch 11/15\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.07017 to 0.04945, saving model to simple_seq2seq_checkpoint.h5\n",
      " - 18s - loss: 0.0643 - val_loss: 0.0495\n",
      "Epoch 12/15\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.04945 to 0.03598, saving model to simple_seq2seq_checkpoint.h5\n",
      " - 18s - loss: 0.0508 - val_loss: 0.0360\n",
      "Epoch 13/15\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.03598 to 0.02982, saving model to simple_seq2seq_checkpoint.h5\n",
      " - 18s - loss: 0.0357 - val_loss: 0.0298\n",
      "Epoch 14/15\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02982 to 0.02100, saving model to simple_seq2seq_checkpoint.h5\n",
      " - 18s - loss: 0.0383 - val_loss: 0.0210\n",
      "Epoch 15/15\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.02100 to 0.01514, saving model to simple_seq2seq_checkpoint.h5\n",
      " - 18s - loss: 0.0223 - val_loss: 0.0151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fa79a25c6a0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtUAAAGJCAYAAABFF896AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wHZyLU6x8iGowAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa79a4e5f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = simple_seq2seq.fit(X_train, np.expand_dims(Y_train, -1),\n",
    "                             validation_data=(X_val, np.expand_dims(Y_val, -1)),\n",
    "                             epochs=15, verbose=2, batch_size=32,\n",
    "                             callbacks=[best_model_cb])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], '--', label='validation')\n",
    "plt.ylabel('negative log likelihood')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('Convergence plot for Simple Seq2Seq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's load the best model found on the validation set at the end of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "simple_seq2seq = load_model(best_model_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If you don't have access to a GPU and cannot wait 10 minutes to for the model to converge to a reasonably good state, feel to use the pretrained model. This model has been obtained by training the above model for ~150 epochs. The validation loss is significantly lower than 1e-5. In practice it should hardly ever make any prediction error on this easy translation problem.\n",
    "\n",
    "Alternatively we will load this imperfect model (trained only to 50 epochs) with a validation loss of ~7e-4. This model makes funny translation errors so I would suggest to try it first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "filename = get_file(\n",
    "    \"simple_seq2seq_partially_pretrained.h5\", \n",
    "    \"https://github.com/m2dsupsdlclass/lectures-labs/releases/\"\n",
    "    \"download/0.4/simple_seq2seq_partially_pretrained.h5\"\n",
    ")\n",
    "\n",
    "# Uncomment the following to replace for the fully trained network:\n",
    "\n",
    "filename= get_file(\n",
    "     \"simple_seq2seq_pretrained.h5\", \n",
    "     \"https://github.com/m2dsupsdlclass/lectures-labs/releases/\"\n",
    "     \"download/0.4/simple_seq2seq_pretrained.h5\")\n",
    "\n",
    "simple_seq2seq.load_weights(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's have a look at a raw prediction on the first sample of the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'quatre vingt seize mille deux cent trente sept'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In numeric array this is provided (along with the expected target sequence) as the following padded input sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0, 31, 35, 14, 18, 23, 30, 38, 28,  1, 13,\n",
       "        10,  6,  7, 11]], dtype=int32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_test_sequence = X_test[0:1]\n",
    "first_test_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Remember that the `_GO` (symbol indexed at `1`) separates the reversed source from the expected target sequence:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_GO'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_shared_vocab[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Interpreting the model prediction\n",
    "\n",
    "**Exercise **:\n",
    "- Feed this test sequence into the model. What is the shape of the output?\n",
    "- Get the argmax of each output prediction to get the most likely symbols\n",
    "- Dismiss the padding / end of sentence\n",
    "- Convert to readable vocabulary using rev_shared_vocab\n",
    "\n",
    "*Interpretation*\n",
    "- Compare the output with the first example in numerical format `num_test[0]`\n",
    "- What do you think of this way of decoding? Is it correct to use it at inference time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = simple_seq2seq.predict(first_test_sequence)\n",
    "print(\"prediction shape:\", prediction.shape)\n",
    "\n",
    "# Let's use `argmax` to extract the predicted token ids at each step:\n",
    "predicted_token_ids = prediction[0].argmax(-1)\n",
    "print(\"prediction token ids:\", predicted_token_ids)\n",
    "\n",
    "# We can use the shared reverse vocabulary to map \n",
    "# this back to the string representation of the tokens,\n",
    "# as well as removing Padding and EOS symbols\n",
    "predicted_numbers = [rev_shared_vocab[token_id] for token_id in predicted_token_ids\n",
    "                     if token_id not in (shared_vocab[PAD], shared_vocab[EOS])]\n",
    "print(\"predicted number:\", \"\".join(predicted_numbers))\n",
    "print(\"test number:\", num_test[0])\n",
    "\n",
    "# The model successfully predicted the test sequence.\n",
    "# However, we provided the full sequence as input, including all the solution\n",
    "# (except for the last number). In a real testing condition, one wouldn't\n",
    "# have the full input sequence, but only what is provided before the \"GO\"\n",
    "# symbol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the previous exercise we cheated because we gave the complete sequence along with the solution in the input sequence.\n",
    "\n",
    "To be more realistic we need to use the model in a setting where we do not provide any token of expected translation as part of the input sequence: the model shall predict one token at a time starting only from the source sequence along with the `<GO>` special symbol. At each step, we append the new predicted output token in the input sequence to predict the next token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def greedy_translate(model, source_sequence, shared_vocab, rev_shared_vocab,\n",
    "                     word_level_source=True, word_level_target=True):\n",
    "    \"\"\"Greedy decoder recursively predicting one token at a time\"\"\"\n",
    "    # Initialize the list of input token ids with the source sequence\n",
    "    source_tokens = tokenize(source_sequence, word_level=word_level_source)\n",
    "    input_ids = [shared_vocab.get(t, UNK) for t in reversed(source_tokens)]\n",
    "    input_ids += [shared_vocab[GO]]\n",
    "\n",
    "    # Prepare a fixed size numpy array that matches the expected input\n",
    "    # shape for the model\n",
    "    input_array = np.empty(shape=(1, model.input_shape[1]),\n",
    "                           dtype=np.int32)\n",
    "    decoded_tokens = []\n",
    "    while len(input_ids) <= max_length:\n",
    "        # Vectorize a the list of input tokens as \n",
    "        # and use zeros padding.\n",
    "        input_array.fill(shared_vocab[PAD])\n",
    "        input_array[0, -len(input_ids):] = input_ids\n",
    "        \n",
    "        # Predict the next output: greedy decoding with argmax\n",
    "        next_token_id = model.predict(input_array)[0, -1].argmax()\n",
    "        \n",
    "        # Stop decoding if the network predicts end of sentence:\n",
    "        if next_token_id == shared_vocab[EOS]:\n",
    "            break\n",
    "            \n",
    "        # Otherwise use the reverse vocabulary to map the prediction\n",
    "        # back to the string space\n",
    "        decoded_tokens.append(rev_shared_vocab[next_token_id])\n",
    "        \n",
    "        # Append prediction to input sequence to predict the next\n",
    "        input_ids.append(next_token_id)\n",
    "\n",
    "    separator = \" \" if word_level_target else \"\"\n",
    "    return separator.join(decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "phrases = [\n",
    "    \"un\",\n",
    "    \"deux\",\n",
    "    \"trois\",\n",
    "    \"onze\",\n",
    "    \"quinze\",\n",
    "    \"cent trente deux\",\n",
    "    \"cent mille douze\",\n",
    "    \"sept mille huit cent cinquante neuf\",\n",
    "    \"vingt et un\",\n",
    "    \"vingt quatre\",\n",
    "    \"quatre vingts\",\n",
    "    \"quatre vingt onze mille\",\n",
    "    \"quatre vingt onze mille deux cent deux\",\n",
    "]\n",
    "for phrase in phrases:\n",
    "    translation = greedy_translate(simple_seq2seq, phrase,\n",
    "                                   shared_vocab, rev_shared_vocab,\n",
    "                                   word_level_target=False)\n",
    "    print(phrase.ljust(40), translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are far from perfect but we can see that the network has already picked up some translation skills.\n",
    "\n",
    "Why does the partially trained network is able to give the correct translation for:\n",
    "\n",
    "`\"sept mille huit cent cinquante neuf\"`\n",
    "\n",
    "but not for:\n",
    "\n",
    "`\"cent mille douze\"` ?\n",
    "\n",
    "The answer is as following:\n",
    "- it is rather easy for the network to learn a correspondance between symbols (first case), by dismissing `\"cent\"` and `\"mille\"`\n",
    "- outputing the right amount of symbols, especially `0s` for `\"cent mille douze\"` requires more reasoning and ability to count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "phrases = [\n",
    "    \"quatre vingt et un\",\n",
    "    \"quarante douze\",\n",
    "    \"onze mille soixante vingt sept\",\n",
    "    \"deux mille soixante vingt quatorze\",\n",
    "]\n",
    "for phrase in phrases:\n",
    "    translation = greedy_translate(simple_seq2seq, phrase,\n",
    "                                   shared_vocab, rev_shared_vocab,\n",
    "                                   word_level_target=False)\n",
    "    print(phrase.ljust(40), translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model evaluation\n",
    "\n",
    "Because **we expect only one correct translation** for a given source sequence, we can use **phrase-level accuracy** as a metric to quantify our model quality.\n",
    "\n",
    "Note that **this is not the case for real translation models** (e.g. from French to English on arbitrary sentences). Evaluation of a machine translation model is tricky in general. Automated evaluation can somehow be done at the corpus level with the [BLEU score](https://en.wikipedia.org/wiki/BLEU) (bilingual evaluation understudy) given a large enough sample of correct translations provided by certified translators but its only a noisy proxy.\n",
    "\n",
    "The only good evaluation is to give a large enough sample of the model predictions on some test sentences to certified translators and ask them to give an evaluation (e.g. a score between 0 and 6, 0 for non-sensical and 6 for the hypothetical perfect translation). However in practice this is very costly to do.\n",
    "\n",
    "Fortunately we can just use phrase-level accuracy on a our very domain specific toy problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def phrase_accuracy(model, num_sequences, fr_sequences, n_samples=300,\n",
    "                    decoder_func=greedy_translate):\n",
    "    correct = []\n",
    "    n_samples = len(num_sequences) if n_samples is None else n_samples\n",
    "    for i, num_seq, fr_seq in zip(range(n_samples), num_sequences, fr_sequences):\n",
    "        if i % 100 == 0:\n",
    "            print(\"Decoding %d/%d\" % (i, n_samples))\n",
    "\n",
    "        predicted_seq = decoder_func(simple_seq2seq, fr_seq,\n",
    "                                     shared_vocab, rev_shared_vocab,\n",
    "                                     word_level_target=False)\n",
    "        correct.append(num_seq == predicted_seq)\n",
    "    return np.mean(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Phrase-level test accuracy: %0.3f\"\n",
    "      % phrase_accuracy(simple_seq2seq, num_test, fr_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Phrase-level train accuracy: %0.3f\"\n",
    "      % phrase_accuracy(simple_seq2seq, num_train, fr_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Bonus: Decoding with a Beam Search\n",
    "\n",
    "Instead of decoding with greedy strategy that only considers the most-likely next token at each prediction, we can hold a priority queue of the most promising top-n sequences ordered by loglikelihoods.\n",
    "\n",
    "This could potentially improve the final accuracy of an imperfect model: indeed it can be the case that the most likely sequence (based on the conditional proability estimated by the model) starts with a character that is not the most likely alone.\n",
    "\n",
    "**Bonus Exercise:**\n",
    "- build a beam_translate function which decodes candidate translations with a beam search strategy\n",
    "- use a list of candidates, tracking `beam_size` candidates and their corresponding likelihood\n",
    "- compute predictions for the next outputs by using predict with a batch of the size of the beam\n",
    "- be careful to stop appending results if EOS symbols have been found for each candidate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def beam_translate(model, source_sequence, shared_vocab, rev_shared_vocab,\n",
    "                   word_level_source=True, word_level_target=True,\n",
    "                   beam_size=10, return_ll=False):\n",
    "    \"\"\"Decode candidate translations with a beam search strategy\n",
    "    \n",
    "    If return_ll is False, only the best candidate string is returned.\n",
    "    If return_ll is True, all the candidate strings and their loglikelihoods\n",
    "    are returned.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# %load beam_search.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "candidates = beam_translate(simple_seq2seq, \"cent mille un\",\n",
    "                            shared_vocab, rev_shared_vocab,\n",
    "                            word_level_target=False,\n",
    "                            return_ll=True, beam_size=10)\n",
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "candidates = beam_translate(simple_seq2seq, \"quatre vingts\",\n",
    "                            shared_vocab, rev_shared_vocab,\n",
    "                            word_level_target=False,\n",
    "                            return_ll=True, beam_size=10)\n",
    "candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model Accuracy with Beam Search Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Phrase-level test accuracy: %0.3f\"\n",
    "      % phrase_accuracy(simple_seq2seq, num_test, fr_test,\n",
    "                        decoder_func=beam_translate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Phrase-level train accuracy: %0.3f\"\n",
    "      % phrase_accuracy(simple_seq2seq, num_train, fr_train,\n",
    "                        decoder_func=beam_translate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "When using the partially trained model the test phrase-level is slightly better (0.38 vs 0.37) with the beam decoder than with the greedy decoder. However the improvement is not that important on our toy task. Training the model to convergence would yield a perfect score on the test set anyway.\n",
    "\n",
    "Properly tuned beam search decoding can be critical to improve the quality of Machine Translation systems trained on natural language pairs though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Going Further\n",
    "\n",
    "We only scratched the surface of sequence-to-sequence systems. To go further, we recommend reading the initial [Sequence to Sequence paper](https://arxiv.org/abs/1409.3215) as well as the following developments, citing this work. Furthermore, here are a few pointers on how to go further if you're interested.\n",
    "\n",
    "### Improved model\n",
    "\n",
    "- Add multiple, larger GRU layers and more dropout regularization.\n",
    "- This should make it possible train a perfect translation model with a smaller amount of labeled samples. Try to train a seq2seq model with only 4000 training sequences or even fewer without overfitting.\n",
    "- You will need a GPU and more training time for that.\n",
    "\n",
    "### Reverse translation: Numeric to French\n",
    "\n",
    "- Build a model, with the same data from Numeric to French\n",
    "- The model should fine work with the same kind of architecture\n",
    "\n",
    "\n",
    "### Separated Encoder-Decoder\n",
    "\n",
    "We may want to build a model with a separated encoder and decoder, to improve performance and be more flexible with the architecture.\n",
    "\n",
    "With Keras, you can get access to the activation states of the LSTM cell by using:\n",
    "\n",
    "```python\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "```\n",
    "\n",
    "then you can reuse those states to initialize the state of the decoder cell:\n",
    "\n",
    "```python\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "```\n",
    "\n",
    "A full example of this architecture can be found at:\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n",
    "\n",
    "**Exercise**:\n",
    "\n",
    "- Implement the Encoder-Decoder Seq2Seq architecture an apply it to the French numbers dataset.\n",
    "\n",
    "### Attention models\n",
    "\n",
    "Instead of initializing the decoder with the encoder states, it is possible to use an attention-model.\n",
    "\n",
    "At the time of writing Keras does not yet have an turn-key attentional mechanism layer but it's possible to build one by assembling lower level building blocks. For instance see:\n",
    "\n",
    "- https://github.com/keras-team/keras/issues/1472#issuecomment-172095544\n",
    "\n",
    "Attention models are efficient to model longer sequences, to find alignment between input and output sequences, and to model different parts of sequences with seperated meanings\n",
    "\n",
    "Other frameworks also have working examples of attention mechanisms:\n",
    "\n",
    "- A good implementation is available for translation here: http://opennmt.net/PythonGuide/\n",
    "- TensorFlow also has working examples: https://www.tensorflow.org/tutorials/seq2seq\n",
    "\n",
    "\n",
    "### Mastering Neural Machine Translation\n",
    "\n",
    "In complement to studying the TensorFlow seq2seq and OpenNMT code base, you might also want to read the following 55 pages tutorial:\n",
    "\n",
    "[Neural Machine Translation and Sequence-to-sequence Models: A Tutorial](https://arxiv.org/abs/1703.01619) by Graham Neubig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML]",
   "language": "python",
   "name": "conda-env-ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
