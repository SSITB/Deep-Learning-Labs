{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation and Multilayer Neural Networks\n",
    "\n",
    "### Goals: \n",
    "- Intro: train a neural network with high level framework `Keras`\n",
    "\n",
    "### Dataset:\n",
    "- Digits: 10 class handwritten digits\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "# display figures in the notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119143668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_index = 44\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(digits.images[sample_index], cmap=plt.cm.gray_r,\n",
    "           interpolation='nearest')\n",
    "plt.title(\"image label: %d\" % digits.target[sample_index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "- normalization\n",
    "- train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0000000e+00 2.9862475e-01 5.1925344e+00 1.1842829e+01 1.1856582e+01\n",
      " 5.8100853e+00 1.3477407e+00 1.1918795e-01 6.5487884e-03 1.9934512e+00\n",
      " 1.0346431e+01 1.1930583e+01 1.0262607e+01 8.2049770e+00 1.8434839e+00\n",
      " 9.8886706e-02 3.2743942e-03 2.6162410e+00 9.8349705e+00 6.8709888e+00\n",
      " 7.1126390e+00 7.8375902e+00 1.7537656e+00 4.4531763e-02 1.3097577e-03\n",
      " 2.4878848e+00 9.0445318e+00 8.7328091e+00 9.9823179e+00 7.5710545e+00\n",
      " 2.2685003e+00 2.6195154e-03 0.0000000e+00 2.3614931e+00 7.6836934e+00\n",
      " 9.0792408e+00 1.0371316e+01 8.7924032e+00 2.8998036e+00 0.0000000e+00\n",
      " 8.5134255e-03 1.5703995e+00 6.8493776e+00 7.2285528e+00 7.6502948e+00\n",
      " 8.2652264e+00 3.4859202e+00 2.6195154e-02 8.5134255e-03 6.8369353e-01\n",
      " 7.4931240e+00 9.5651608e+00 9.3686972e+00 8.7766867e+00 3.7668631e+00\n",
      " 2.0890635e-01 6.5487885e-04 2.7897838e-01 5.5370007e+00 1.2059594e+01\n",
      " 1.1783236e+01 6.8199081e+00 2.0746562e+00 3.4250164e-01]\n",
      "[1.         0.91047853 4.737475   4.259608   4.3074083  5.682445\n",
      " 3.2971535  1.0088968  0.10215284 3.2197208  5.410791   3.9943264\n",
      " 4.763272   6.0576067  3.5449781  0.78794795 0.06762655 3.5953584\n",
      " 5.7559114  5.7949924  6.1700764  6.203925   3.1713367  0.4243509\n",
      " 0.03616688 3.1758604  6.227161   5.8784328  6.1276746  5.886606\n",
      " 3.6396353  0.05111425 1.         3.5114381  6.3518367  6.2620006\n",
      " 5.923798   5.862816   3.5431352  1.         0.13754639 2.9667552\n",
      " 6.5228195  6.4634943  6.2735844  5.6645765  4.3256125  0.31021976\n",
      " 0.22145751 1.7102222  5.6299205  5.2322054  5.31296    6.0307407\n",
      " 4.9404445  0.9965206  0.02558208 0.9406645  5.0750594  4.379795\n",
      " 4.9665     5.9209824  4.0502324  1.7806677 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = np.asarray(digits.data, dtype='float32')\n",
    "target = np.asarray(digits.target, dtype='int32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.15, random_state=37)\n",
    "\n",
    "# mean = 0 ; standard deviation = 1.0\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(scaler.mean_)\n",
    "print(scaler.scale_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the one of the transformed sample (after feature standardization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11921e4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_index = 45\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(X_train[sample_index].reshape(8, 8),\n",
    "           cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.title(\"transformed sample\\n(standardization)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaler objects makes it possible to recover the original sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11927c908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(scaler.inverse_transform(X_train[sample_index]).reshape(8, 8),\n",
    "           cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.title(\"original sample\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1527, 64) (1527,)\n",
      "(270, 64) (270,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I) Feed Forward NN with Keras\n",
    "\n",
    "Objectives of this section:\n",
    "\n",
    "- Build and train a first feedforward network using `Keras`\n",
    "    - https://keras.io/getting-started/sequential-model-guide/\n",
    "- Experiment with different optimizers, activations, size of layers, initializations\n",
    "\n",
    "### a) Keras Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a first neural network we need to turn the target variable into a vector \"one-hot-encoding\" representation. Here are the labels of the first samples in the training set encoded as integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 9, 5], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides a utility function to convert integer-encoded categorical variables as one-hot encoded values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imadelhanafi/anaconda/envs/ML/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/Users/imadelhanafi/anaconda/envs/ML/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.contrib import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "Y_train = to_categorical(y_train)\n",
    "Y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1527, 64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build an train a our first feed forward neural network using the high level API from keras:\n",
    "\n",
    "- first we define the model by stacking layers with the right dimensions\n",
    "- then we define a loss function and plug the SGD optimizer\n",
    "- then we feed the model the training data for fixed number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1527/1527 [==============================] - 0s 324us/step - loss: 0.3280 - acc: 0.8939\n",
      "Epoch 2/20\n",
      "1527/1527 [==============================] - 0s 64us/step - loss: 0.0914 - acc: 0.9718\n",
      "Epoch 3/20\n",
      "1527/1527 [==============================] - 0s 62us/step - loss: 0.0416 - acc: 0.9921\n",
      "Epoch 4/20\n",
      "1527/1527 [==============================] - 0s 63us/step - loss: 0.0217 - acc: 0.9980\n",
      "Epoch 5/20\n",
      "1527/1527 [==============================] - 0s 74us/step - loss: 0.0142 - acc: 0.9980\n",
      "Epoch 6/20\n",
      "1527/1527 [==============================] - 0s 72us/step - loss: 0.0089 - acc: 1.0000\n",
      "Epoch 7/20\n",
      "1527/1527 [==============================] - 0s 76us/step - loss: 0.0070 - acc: 1.0000\n",
      "Epoch 8/20\n",
      "1527/1527 [==============================] - 0s 71us/step - loss: 0.0055 - acc: 1.0000\n",
      "Epoch 9/20\n",
      "1527/1527 [==============================] - 0s 66us/step - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 10/20\n",
      "1527/1527 [==============================] - 0s 67us/step - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 11/20\n",
      "1527/1527 [==============================] - 0s 62us/step - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 12/20\n",
      "1527/1527 [==============================] - 0s 66us/step - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 13/20\n",
      "1527/1527 [==============================] - 0s 78us/step - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 14/20\n",
      "1527/1527 [==============================] - 0s 71us/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 15/20\n",
      "1527/1527 [==============================] - 0s 73us/step - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 16/20\n",
      "1527/1527 [==============================] - 0s 75us/step - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 17/20\n",
      "1527/1527 [==============================] - 0s 67us/step - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 18/20\n",
      "1527/1527 [==============================] - 0s 65us/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 19/20\n",
      "1527/1527 [==============================] - 0s 75us/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 20/20\n",
      "1527/1527 [==============================] - 0s 71us/step - loss: 0.0017 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras import optimizers\n",
    "\n",
    "N = X_train.shape[1]\n",
    "H = 100\n",
    "K = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(H, input_dim=N))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(Dense(K))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=optimizers.SGD(lr=1),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=20, batch_size=32);\n",
    "# Batch ---> Augmente : saturation de mémoire \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Exercises: Impact of the Optimizer\n",
    "\n",
    "- Try to decrease the learning rate value by 10 or 100. What do you observe?\n",
    "\n",
    "- Try to increase the learning rate value to make the optimization diverge.\n",
    "\n",
    "- Configure the SGD optimizer to enable a Nesterov momentum of 0.9\n",
    "  \n",
    "Note that the keras API documentation is avaiable at:\n",
    "\n",
    "https://keras.io/\n",
    "\n",
    "It is also possible to learn more about the parameters of a class by using the question mark: type and evaluate:\n",
    "\n",
    "```python\n",
    "optimizers.SGD?\n",
    "```\n",
    "\n",
    "in a jupyter notebook cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Stochastic gradient descent optimizer.\n",
       "\n",
       "Includes support for momentum,\n",
       "learning rate decay, and Nesterov momentum.\n",
       "\n",
       "# Arguments\n",
       "    lr: float >= 0. Learning rate.\n",
       "    momentum: float >= 0. Parameter updates momentum.\n",
       "    decay: float >= 0. Learning rate decay over each update.\n",
       "    nesterov: boolean. Whether to apply Nesterov momentum.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda/envs/ML/lib/python3.6/site-packages/keras/optimizers.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizers.SGD?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Replace the SGD optimizer by the Adam optimizer from keras and run it\n",
    "  with the default parameters.\n",
    "\n",
    "- Add another hidden layer and use the \"Rectified Linear Unit\" for each\n",
    "  hidden layer. Can you still train the model with Adam with its default global\n",
    "  learning rate?\n",
    "\n",
    "- Bonus: try the Adadelta optimizer (no learning rate to set).\n",
    "\n",
    "Hint: use `optimizers.<TAB>` to tab-complete the list of implemented optimizers in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.999\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-08\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Adam optimizer.\n",
       "\n",
       "Default parameters follow those provided in the original paper.\n",
       "\n",
       "# Arguments\n",
       "    lr: float >= 0. Learning rate.\n",
       "    beta_1: float, 0 < beta < 1. Generally close to 1.\n",
       "    beta_2: float, 0 < beta < 1. Generally close to 1.\n",
       "    epsilon: float >= 0. Fuzz factor.\n",
       "    decay: float >= 0. Learning rate decay over each update.\n",
       "\n",
       "# References\n",
       "    - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda/envs/ML/lib/python3.6/site-packages/keras/optimizers.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizers.Adam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1527/1527 [==============================] - 1s 660us/step - loss: 1.0284 - acc: 0.7511\n",
      "Epoch 2/20\n",
      "1527/1527 [==============================] - 0s 118us/step - loss: 0.2448 - acc: 0.9561\n",
      "Epoch 3/20\n",
      "1527/1527 [==============================] - 0s 130us/step - loss: 0.1283 - acc: 0.9751\n",
      "Epoch 4/20\n",
      "1527/1527 [==============================] - 0s 120us/step - loss: 0.0798 - acc: 0.9862\n",
      "Epoch 5/20\n",
      "1527/1527 [==============================] - 0s 115us/step - loss: 0.0535 - acc: 0.9915\n",
      "Epoch 6/20\n",
      "1527/1527 [==============================] - 0s 121us/step - loss: 0.0375 - acc: 0.9961\n",
      "Epoch 7/20\n",
      "1527/1527 [==============================] - 0s 124us/step - loss: 0.0265 - acc: 0.9987\n",
      "Epoch 8/20\n",
      "1527/1527 [==============================] - 0s 127us/step - loss: 0.0197 - acc: 0.9987\n",
      "Epoch 9/20\n",
      "1527/1527 [==============================] - 0s 129us/step - loss: 0.0148 - acc: 1.0000\n",
      "Epoch 10/20\n",
      "1527/1527 [==============================] - 0s 128us/step - loss: 0.0116 - acc: 1.0000\n",
      "Epoch 11/20\n",
      "1527/1527 [==============================] - 0s 129us/step - loss: 0.0095 - acc: 1.0000\n",
      "Epoch 12/20\n",
      "1527/1527 [==============================] - 0s 129us/step - loss: 0.0076 - acc: 1.0000\n",
      "Epoch 13/20\n",
      "1527/1527 [==============================] - 0s 132us/step - loss: 0.0064 - acc: 1.0000\n",
      "Epoch 14/20\n",
      "1527/1527 [==============================] - 0s 126us/step - loss: 0.0054 - acc: 1.0000\n",
      "Epoch 15/20\n",
      "1527/1527 [==============================] - 0s 130us/step - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 16/20\n",
      "1527/1527 [==============================] - 0s 129us/step - loss: 0.0039 - acc: 1.0000\n",
      "Epoch 17/20\n",
      "1527/1527 [==============================] - 0s 126us/step - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 18/20\n",
      "1527/1527 [==============================] - 0s 131us/step - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 19/20\n",
      "1527/1527 [==============================] - 0s 127us/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 20/20\n",
      "1527/1527 [==============================] - 0s 155us/step - loss: 0.0023 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Using the Adam optimizer\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(H, input_dim=N))\n",
    "model1.add(Activation(\"tanh\"))\n",
    "\n",
    "model1.add(Dense(H))\n",
    "model1.add(Activation(\"relu\"))\n",
    "\n",
    "model1.add(Dense(K))\n",
    "model1.add(Activation(\"softmax\"))\n",
    "\n",
    "model1.compile(optimizer=optimizers.Adam(),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model1.fit(X_train, Y_train, epochs=20, batch_size=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1527/1527 [==============================] - 1s 519us/step - loss: 1.6326 - acc: 0.5639\n",
      "Epoch 2/15\n",
      "1527/1527 [==============================] - 0s 81us/step - loss: 0.8271 - acc: 0.8612\n",
      "Epoch 3/15\n",
      "1527/1527 [==============================] - 0s 75us/step - loss: 0.5071 - acc: 0.9103\n",
      "Epoch 4/15\n",
      "1527/1527 [==============================] - 0s 77us/step - loss: 0.3524 - acc: 0.9417\n",
      "Epoch 5/15\n",
      "1527/1527 [==============================] - 0s 77us/step - loss: 0.2660 - acc: 0.9561\n",
      "Epoch 6/15\n",
      "1527/1527 [==============================] - 0s 77us/step - loss: 0.2095 - acc: 0.9666\n",
      "Epoch 7/15\n",
      "1527/1527 [==============================] - 0s 78us/step - loss: 0.1717 - acc: 0.9725\n",
      "Epoch 8/15\n",
      "1527/1527 [==============================] - 0s 81us/step - loss: 0.1436 - acc: 0.9751\n",
      "Epoch 9/15\n",
      "1527/1527 [==============================] - 0s 78us/step - loss: 0.1221 - acc: 0.9797\n",
      "Epoch 10/15\n",
      "1527/1527 [==============================] - 0s 79us/step - loss: 0.1054 - acc: 0.9836\n",
      "Epoch 11/15\n",
      "1527/1527 [==============================] - 0s 79us/step - loss: 0.0920 - acc: 0.9862\n",
      "Epoch 12/15\n",
      "1527/1527 [==============================] - 0s 79us/step - loss: 0.0805 - acc: 0.9882\n",
      "Epoch 13/15\n",
      "1527/1527 [==============================] - 0s 77us/step - loss: 0.0712 - acc: 0.9882\n",
      "Epoch 14/15\n",
      "1527/1527 [==============================] - 0s 84us/step - loss: 0.0628 - acc: 0.9895\n",
      "Epoch 15/15\n",
      "1527/1527 [==============================] - 0s 95us/step - loss: 0.0560 - acc: 0.9895\n"
     ]
    }
   ],
   "source": [
    "# Using the Adadelta optimizer\n",
    "# Adaptative Learning Rate\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(H, input_dim=N))\n",
    "model1.add(Activation(\"tanh\"))\n",
    "model1.add(Dense(H))\n",
    "model1.add(Activation(\"relu\"))\n",
    "model1.add(Dense(K))\n",
    "model1.add(Activation(\"softmax\"))\n",
    "\n",
    "model1.compile(optimizer=optimizers.Adadelta(),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model1.fit(X_train, Y_train, epochs=15, batch_size=32);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Exercises: forward pass and generalization\n",
    "\n",
    "- Compute predictions on test set using `model.predict_classes(...)`\n",
    "- Compute average accuracy of the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.9814814814814815\n"
     ]
    }
   ],
   "source": [
    "Predictions = model1.predict_classes(X_test)\n",
    "True_False = [Predictions[i] == y_test[i] for i in range(len(Predictions))]\n",
    "\n",
    "acc = (len(y_test) - sum(True_False))/(len(y_test))\n",
    "print(\"Accuracy : \", 1-acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9814814814814815"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(True_False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.predict_classes(X_test, verbose=0)\n",
    "\n",
    "# Let's display the first inputs image, the predicted labels and the true labels\n",
    "plt.figure(figsize=(12, 9))\n",
    "for i in range(15):\n",
    "    plt.subplot(3, 5, i + 1)\n",
    "    plt.imshow(scaler.inverse_transform(X_test[i]).reshape(8, 8),\n",
    "               cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title(\"predicted label: %d\\n true label: %d\"\n",
    "              % (y_predicted[i], y_test[i]))\n",
    "    \n",
    "print(\"test acc: %0.4f\" % np.mean(y_predicted == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute the conditional probabilities of sample number 42 of the test set with `model.predict_proba(...)`\n",
    "- Derive the loss (negative log likelihood of that sample) using numpy operations\n",
    "- Compute the average negative log likelihood of the test set.\n",
    "- Compare this value to the training loss reported by keras: is the model overfitting or underfitting?\n",
    "\n",
    "Note: you might need to retrain the model with a larger number of epochs (e.g. 50) to ensure that it has fully converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probability distribution for sample #42:\n",
      "0: 0.00000\n",
      "1: 0.00000\n",
      "2: 0.00000\n",
      "3: 0.00000\n",
      "4: 0.00000\n",
      "5: 0.00000\n",
      "6: 0.00000\n",
      "7: 0.00000\n",
      "8: 1.00000\n",
      "9: 0.00000\n",
      "\n",
      "Likelihood of true class for sample #42:\n",
      "0.99999666\n",
      "\n",
      "Average negative loglikelihood of the test set:\n",
      "0.0791550256489166\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12f961828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_idx = 42\n",
    "plt.imshow(scaler.inverse_transform(X_test[sample_idx]).reshape(8, 8),\n",
    "           cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.title(\"true label: %d\" % y_test[sample_idx])\n",
    "\n",
    "# Compute all probabilities for all samples in the test set at\n",
    "# once:\n",
    "probabilities = model.predict_proba(X_test, verbose=0)\n",
    "\n",
    "print(\"Predicted probability distribution for sample #42:\")\n",
    "for class_idx, prob in enumerate(probabilities[sample_idx]):\n",
    "    print(\"%d: %0.5f\" % (class_idx, prob))\n",
    "print()\n",
    "    \n",
    "print(\"Likelihood of true class for sample #42:\")\n",
    "print(probabilities[sample_idx, y_test[sample_idx]])\n",
    "print()\n",
    "\n",
    "print(\"Average negative loglikelihood of the test set:\")\n",
    "Y_test = to_categorical(y_test)\n",
    "loglikelihoods = np.sum(np.log(probabilities) * Y_test, axis=1)\n",
    "print(-np.mean(loglikelihoods))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Home assignment: impact of initialization\n",
    "\n",
    "Let us now study the impact of a bad initialization when training\n",
    "a deep feed forward network.\n",
    "\n",
    "By default Keras dense layers use the \"Glorot Uniform\" initialization\n",
    "strategy to initialize the weight matrices:\n",
    "\n",
    "- each weight coefficient is randomly sampled from [-scale, scale]\n",
    "- scale is proportional to $\\frac{1}{\\sqrt{n_{in} + n_{out}}}$\n",
    "\n",
    "This strategy is known to work well to initialize deep neural networks\n",
    "with \"tanh\" or \"relu\" activation functions and then trained with\n",
    "standard SGD.\n",
    "\n",
    "To assess the impact of initialization let us plug an alternative init\n",
    "scheme into a 2 hidden layers networks with \"tanh\" activations.\n",
    "For the sake of the example let's use normal distributed weights\n",
    "with a manually adjustable scale (standard deviation) and see the\n",
    "impact the scale value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import initializers\n",
    "\n",
    "normal_init = initializers.RandomNormal(stddev=0.01)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(H, input_dim=N, kernel_initializer=normal_init))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(Dense(K, kernel_initializer=normal_init))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(Dense(K, kernel_initializer=normal_init))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=optimizers.SGD(lr=0.1),\n",
    "              loss='categorical_crossentropy')\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "\n",
    "- Try the following initialization schemes and see whether\n",
    "  the SGD algorithm can successfully train the network or\n",
    "  not:\n",
    "  \n",
    "  - a very small e.g. `scale=1e-3`\n",
    "  - a larger scale e.g. `scale=1` or `10`\n",
    "  - initialize all weights to 0 (constant initialization)\n",
    "  \n",
    "- What do you observe? Can you find an explanation for those\n",
    "  outcomes?\n",
    "\n",
    "- Are better solvers such as SGD with momentum or Adam able\n",
    "  to deal better with such bad initializations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_scale_init = initializers.RandomNormal(stddev=1)\n",
    "small_scale_init = initializers.RandomNormal(stddev=1e-3)\n",
    "\n",
    "\n",
    "optimizer_list = [\n",
    "    ('SGD', optimizers.SGD(lr=0.1)),\n",
    "    ('Adam', optimizers.Adam()),\n",
    "#     ('SGD + Nesterov momentum', optimizers.SGD(\n",
    "#             lr=0.1, momentum=0.9,nesterov=True)),\n",
    "]\n",
    "\n",
    "init_list = [\n",
    "    ('glorot uniform init', 'glorot_uniform', '-'),\n",
    "    ('small init scale', small_scale_init, '-'),\n",
    "    ('large init scale', large_scale_init, '-'),\n",
    "    ('zero init', 'zero', '--'),\n",
    "]\n",
    "\n",
    "\n",
    "for optimizer_name, optimizer in optimizer_list:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for init_name, init, linestyle in init_list:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(H, input_dim=N, kernel_initializer=init))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "        model.add(Dense(K, kernel_initializer=init))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "        model.add(Dense(K, kernel_initializer=init))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss='categorical_crossentropy')\n",
    "\n",
    "        history = model.fit(X_train, Y_train,\n",
    "                            epochs=10, batch_size=32, verbose=0)\n",
    "        plt.plot(history.history['loss'], linestyle=linestyle,\n",
    "                 label=init_name)\n",
    "\n",
    "    plt.xlabel('# epochs')\n",
    "    plt.ylabel('Training loss')\n",
    "    plt.ylim(0, 6)\n",
    "    plt.legend(loc='best');\n",
    "    plt.title('Impact of initialization on convergence with %s'\n",
    "              % optimizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3.6)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
